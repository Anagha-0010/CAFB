{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 300\n",
    "OVERLAP_SIZE = 150\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def split_into_token_chunks(text: str, chunk_size: int = CHUNK_SIZE) -> List[str]:\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "def split_into_token_chunks(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP_SIZE) -> List[str]:\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(tokenizer.decode(chunk_tokens))\n",
    "    return chunks\n",
    "\n",
    "def parse_pdf_date(pdf_date: str) -> str:\n",
    "    match = re.search(r\"D:(\\d{4})(\\d{2})(\\d{2})\", pdf_date)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-{match.group(2)}-{match.group(3)}\"\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_blog_post(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    text = clean_text(doc.get(\"content\", \"\"))\n",
    "    title = doc.get(\"title\", f\"blog_{doc_id}\")\n",
    "    date = doc.get(\"date\", \"\")\n",
    "    chunks = split_into_token_chunks(text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"blog_posts\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i, chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "def chunk_collateral(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    title = doc.get(\"file_name\",f\"collateral_{doc_id}\")\n",
    "    date = parse_pdf_date(doc.get(\"metadata\",{}).get(\"CreationDate\",\"\"))\n",
    "    text_blocks = [item.get(\"text\",\"\")for item in doc.get(\"text_data\",[])]\n",
    "    full_text = clean_text(\" \".join(text_blocks))\n",
    "    chunks = split_into_token_chunks(full_text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"collateral\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i,chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "def chunk_grant_proposal(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    title = doc.get(\"file_name\",f\"grant_{doc_id}\")\n",
    "    date = parse_pdf_date(doc.get(\"metadata\",{}).get(\"CreationDate\",\"\"))\n",
    "    text_blocks = [item.get(\"text\",\"\")for item in doc.get(\"text_data\",[])]\n",
    "    full_text = clean_text(\" \".join(text_blocks))\n",
    "    chunks = split_into_token_chunks(full_text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"collateral\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i,chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "def chunk_powerpoint(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    title = doc.get(\"file_name\", f\"ppt_{doc_id}\")\n",
    "    date = doc.get(\"metadata\", {}).get(\"Created\", \"\")\n",
    "    text_blocks = [item.get(\"text\", \"\") for item in doc.get(\"text_data\", [])]\n",
    "    full_text = clean_text(\" \".join(text_blocks))\n",
    "    chunks = split_into_token_chunks(full_text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"powerpoints\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i, chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "def chunk_video_captions(txt_path: str, title: str, doc_id: str, group_size: int = 5) -> List[Dict]:\n",
    "    with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    filtered = []\n",
    "    for line in lines:\n",
    "        if line.startswith(('WEBVTT', 'Kind:', 'Language:', '<', '00:')):\n",
    "            continue\n",
    "        # Remove embedded timestamp tags like <00:01:02.640><c>\n",
    "        clean = re.sub(r'<.*?>', '', line)\n",
    "        filtered.append(clean)\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for line in filtered:\n",
    "        if line not in seen:\n",
    "            deduped.append(line)\n",
    "            seen.add(line)\n",
    "\n",
    "    grouped_chunks = [\" \".join(deduped[i:i + group_size]) for i in range(0, len(deduped), group_size)]\n",
    "\n",
    "    print(f\"[transcript] {doc_id}: {len(grouped_chunks)} grouped caption chunks from '{title}'\")\n",
    "\n",
    "    return [{\n",
    "        \"text\": clean_text(chunk),\n",
    "        \"source\": \"transcript\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": \"\"\n",
    "    } for i, chunk in enumerate(grouped_chunks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl_file(file_path: str, chunk_fn, source: str, output_path: str):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        docs = [json.loads(line.strip())for line in f if line.strip()]\n",
    "\n",
    "    all_chunks=[]\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_id=f\"{source}_{i:03d}\"\n",
    "        all_chunks.extend(chunk_fn(doc,doc_id))\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in all_chunks:\n",
    "            f.write(json.dumps(chunk) + '\\n')\n",
    "        print(f\"Saved {len(all_chunks)} chunks to {output_path}\")\n",
    "\n",
    "\n",
    "def process_folder(folder_path: str, chunk_fn, source: str, output_path: str):\n",
    "    all_chunks = []\n",
    "    for i, file in enumerate(Path(folder_path).glob(\"*.json\")):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            doc = json.load(f)\n",
    "            doc_id = file.stem\n",
    "            all_chunks.extend(chunk_fn(doc, doc_id))\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in all_chunks:\n",
    "            f.write(json.dumps(chunk) + '\\n')\n",
    "    print(f\"Saved {len(all_chunks)} chunks to {output_path}\")\n",
    "\n",
    "\n",
    "def chunk_all_caption_files(folder_path: str, output_path: str, group_size: int = 5):\n",
    "    all_chunks = []\n",
    "    folder = Path(folder_path)\n",
    "    for i, file in enumerate(folder.glob(\"*.txt\")):\n",
    "        doc_id = file.stem\n",
    "        title = file.stem.replace(\"_\", \" \")\n",
    "        chunks = chunk_video_captions(str(file), title, doc_id, group_size)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in all_chunks:\n",
    "            f.write(json.dumps(chunk) + '\\n')\n",
    "    print(f\"Saved {len(all_chunks)} chunks to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_collateral_images(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    image_name = doc.get(\"image_name\", \"\")\n",
    "    source_pdf = doc.get(\"source_pdf\", \"\")\n",
    "    page_number = doc.get(\"page_number\", None)\n",
    "    image_type = doc.get(\"type\", \"\")\n",
    "    title = Path(source_pdf).stem.replace(\"_\", \" \")\n",
    "\n",
    "    text = f\"{image_type.title()} from page {page_number} of {title}\" if page_number else image_type.title()\n",
    "\n",
    "    return [{\n",
    "        \"text\": text,\n",
    "        \"source\": \"collateral_image\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": 0,\n",
    "        \"date\": \"\",\n",
    "        \"metadata\": {\n",
    "            \"image_name\": image_name,\n",
    "            \"source_pdf\": source_pdf,\n",
    "            \"page_number\": page_number,\n",
    "            \"type\": image_type\n",
    "        }\n",
    "    }]\n",
    "\n",
    "def chunk_powerpoint_images(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    image_name = doc.get(\"image_name\", \"\")\n",
    "    slide_number = doc.get(\"slide_number\", None)\n",
    "    original_ppt = doc.get(\"original_ppt\", \"\")\n",
    "    title = Path(original_ppt).stem.replace(\"_\", \" \")\n",
    "\n",
    "    text = f\"Slide {slide_number} image from {title}\" if slide_number else \"PowerPoint Image\"\n",
    "\n",
    "    return [{\n",
    "        \"text\": text,\n",
    "        \"source\": \"powerpoint_image\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": 0,\n",
    "        \"date\": \"\",\n",
    "        \"metadata\": {\n",
    "            \"image_name\": image_name,\n",
    "            \"original_ppt\": original_ppt,\n",
    "            \"slide_number\": slide_number\n",
    "        }\n",
    "    }]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2309 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_blog.jsonl\n",
      "Saved 177 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_collateral.jsonl\n",
      "Saved 95 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_powerpoint.jsonl\n",
      "Saved 775 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_grants.jsonl\n",
      "[transcript] SH-Bck3d6h8: 5 grouped caption chunks from 'SH-Bck3d6h8'\n",
      "[transcript] cDXvnEhu21c: 4 grouped caption chunks from 'cDXvnEhu21c'\n",
      "[transcript] BnfDzskvnDE: 60 grouped caption chunks from 'BnfDzskvnDE'\n",
      "[transcript] l3MHqmyqXgs: 63 grouped caption chunks from 'l3MHqmyqXgs'\n",
      "[transcript] zfuSsOSY450: 3 grouped caption chunks from 'zfuSsOSY450'\n",
      "[transcript] 7sl9JhOAN40: 35 grouped caption chunks from '7sl9JhOAN40'\n",
      "[transcript] Fn9vYywn_NU: 23 grouped caption chunks from 'Fn9vYywn NU'\n",
      "[transcript] fsj7hnHquNo: 7 grouped caption chunks from 'fsj7hnHquNo'\n",
      "[transcript] B1KujrupuRU: 31 grouped caption chunks from 'B1KujrupuRU'\n",
      "[transcript] 86o1xYk5bTk: 28 grouped caption chunks from '86o1xYk5bTk'\n",
      "[transcript] Dz49C-sBlMQ: 3 grouped caption chunks from 'Dz49C-sBlMQ'\n",
      "[transcript] 48ufxYsK-l4: 18 grouped caption chunks from '48ufxYsK-l4'\n",
      "[transcript] 3Kq7FfmCAD4: 36 grouped caption chunks from '3Kq7FfmCAD4'\n",
      "[transcript] tnowTuVCLVQ: 47 grouped caption chunks from 'tnowTuVCLVQ'\n",
      "[transcript] 1FYjg3-rThs: 38 grouped caption chunks from '1FYjg3-rThs'\n",
      "[transcript] Mc3LBD-aVr8: 68 grouped caption chunks from 'Mc3LBD-aVr8'\n",
      "[transcript] Xu5iE-0hIqc: 18 grouped caption chunks from 'Xu5iE-0hIqc'\n",
      "[transcript] 4gA-wPwhOBM: 2 grouped caption chunks from '4gA-wPwhOBM'\n",
      "[transcript] wnmU57ll7Cc: 6 grouped caption chunks from 'wnmU57ll7Cc'\n",
      "[transcript] 087xN6pXA78: 5 grouped caption chunks from '087xN6pXA78'\n",
      "[transcript] noThPrC5yOg: 5 grouped caption chunks from 'noThPrC5yOg'\n",
      "[transcript] bkmCVXJsGQo: 7 grouped caption chunks from 'bkmCVXJsGQo'\n",
      "[transcript] zNHke8oBHgw: 37 grouped caption chunks from 'zNHke8oBHgw'\n",
      "[transcript] SfpRdJGKm0I: 7 grouped caption chunks from 'SfpRdJGKm0I'\n",
      "[transcript] -kky-eWgPIo: 27 grouped caption chunks from '-kky-eWgPIo'\n",
      "[transcript] s62cmm1HqZY: 20 grouped caption chunks from 's62cmm1HqZY'\n",
      "[transcript] Tz84hxF-OOU: 5 grouped caption chunks from 'Tz84hxF-OOU'\n",
      "[transcript] 4AtfJF6itEs: 12 grouped caption chunks from '4AtfJF6itEs'\n",
      "[transcript] C3vu4XU_DDg: 3 grouped caption chunks from 'C3vu4XU DDg'\n",
      "[transcript] 7xe-G-LDtgU: 48 grouped caption chunks from '7xe-G-LDtgU'\n",
      "[transcript] 1Zc4ujpgIzg: 3 grouped caption chunks from '1Zc4ujpgIzg'\n",
      "[transcript] ZT16TAtdAF0: 20 grouped caption chunks from 'ZT16TAtdAF0'\n",
      "[transcript] zOHBgs7VPyg: 45 grouped caption chunks from 'zOHBgs7VPyg'\n",
      "[transcript] vuuDgm7oQiE: 28 grouped caption chunks from 'vuuDgm7oQiE'\n",
      "[transcript] WYBg7mitFCU: 33 grouped caption chunks from 'WYBg7mitFCU'\n",
      "[transcript] 6rqeKpupO88: 3 grouped caption chunks from '6rqeKpupO88'\n",
      "[transcript] uVTACSBse0g: 34 grouped caption chunks from 'uVTACSBse0g'\n",
      "[transcript] l4hgsvb3VU8: 16 grouped caption chunks from 'l4hgsvb3VU8'\n",
      "[transcript] B-V5gEVqozc: 43 grouped caption chunks from 'B-V5gEVqozc'\n",
      "[transcript] EKBSeaHK5sE: 29 grouped caption chunks from 'EKBSeaHK5sE'\n",
      "[transcript] MuXNoXdt8jI: 35 grouped caption chunks from 'MuXNoXdt8jI'\n",
      "[transcript] LKhH9xaIlSw: 5 grouped caption chunks from 'LKhH9xaIlSw'\n",
      "[transcript] _AGVtuo8kcg: 46 grouped caption chunks from ' AGVtuo8kcg'\n",
      "[transcript] 5X1hGTy1xEg: 18 grouped caption chunks from '5X1hGTy1xEg'\n",
      "[transcript] ddXVatNZQdQ: 3 grouped caption chunks from 'ddXVatNZQdQ'\n",
      "[transcript] IUAkjky59KM: 3 grouped caption chunks from 'IUAkjky59KM'\n",
      "[transcript] JSJQynKWGDc: 33 grouped caption chunks from 'JSJQynKWGDc'\n",
      "[transcript] wH6vEW2ySEI: 14 grouped caption chunks from 'wH6vEW2ySEI'\n",
      "[transcript] mOAV31YHHMY: 6 grouped caption chunks from 'mOAV31YHHMY'\n",
      "[transcript] 3sA6Dfgs9ek: 26 grouped caption chunks from '3sA6Dfgs9ek'\n",
      "Saved 1114 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_captions.jsonl\n",
      "Saved 606 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_collateral_images.jsonl\n",
      "Saved 164 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_ppt_images.jsonl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('/Users/sharvari/Downloads/CAFB_Challenge/outputs', exist_ok=True)\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/blog_posts.jsonl',\n",
    "    chunk_blog_post,\n",
    "    'blog_posts',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_blog.jsonl'\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/collateral.jsonl',\n",
    "    chunk_collateral,\n",
    "    'collateral',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_collateral.jsonl'\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/powerpoints.jsonl',\n",
    "    chunk_powerpoint,\n",
    "    'powerpoints',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_powerpoint.jsonl'\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/grant_proposals.jsonl',\n",
    "    chunk_grant_proposal,\n",
    "    'grant_proposals',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_grants.jsonl'\n",
    ")\n",
    "\n",
    "\n",
    "chunk_all_caption_files(\n",
    "    folder_path=\"/Users/sharvari/Downloads/CAFB_Challenge/data/captions\",\n",
    "    output_path=\"/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_captions.jsonl\"\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/collateral-images.jsonl',\n",
    "    chunk_collateral_images,\n",
    "    'collateral_images',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_collateral_images.jsonl'\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/powerpoints-images.jsonl',\n",
    "    chunk_powerpoint_images,\n",
    "    'powerpoint_images',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_ppt_images.jsonl'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/blog_posts.jsonl',\n",
    "#         chunk_blog_post,\n",
    "#         'blog_posts',\n",
    "#         'outputs/chunks_blog.jsonl'\n",
    "#     )\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/collateral.jsonl',\n",
    "#         chunk_collateral,\n",
    "#         'collateral',\n",
    "#         'outputs/chunks_collateral.jsonl'\n",
    "#     )\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/powerpoints.jsonl',\n",
    "#         chunk_powerpoint,\n",
    "#         'powerpoints',\n",
    "#         'outputs/chunks_powerpoints.jsonl'\n",
    "#     )\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/grants_proposals.jsonl',\n",
    "#         chunk_grant_proposal,\n",
    "#         'grant_proposals',\n",
    "#         'outputs/chunks_grants.jsonl'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
