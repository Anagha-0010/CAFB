{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharvari/Downloads/CAFB_Challenge/cafb-env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 300\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def split_into_token_chunks(text: str, chunk_size: int = CHUNK_SIZE) -> List[str]:\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "def parse_pdf_date(pdf_date: str) -> str:\n",
    "    match = re.search(r\"D:(\\d{4})(\\d{2})(\\d{2})\", pdf_date)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}-{match.group(2)}-{match.group(3)}\"\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_blog_post(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    text = clean_text(doc.get(\"content\", \"\"))\n",
    "    title = doc.get(\"title\", f\"blog_{doc_id}\")\n",
    "    date = doc.get(\"date\", \"\")\n",
    "    chunks = split_into_token_chunks(text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"blog_posts\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i, chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "def chunk_collateral(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    title = doc.get(\"file_name\",f\"collateral_{doc_id}\")\n",
    "    date = parse_pdf_date(doc.get(\"metadata\",{}).get(\"CreationDate\",\"\"))\n",
    "    text_blocks = [item.get(\"text\",\"\")for item in doc.get(\"text_data\",[])]\n",
    "    full_text = clean_text(\" \".join(text_blocks))\n",
    "    chunks = split_into_token_chunks(full_text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"collateral\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i,chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "def chunk_grant_proposal(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    title = doc.get(\"file_name\",f\"grant_{doc_id}\")\n",
    "    date = parse_pdf_date(doc.get(\"metadata\",{}).get(\"CreationDate\",\"\"))\n",
    "    text_blocks = [item.get(\"text\",\"\")for item in doc.get(\"text_data\",[])]\n",
    "    full_text = clean_text(\" \".join(text_blocks))\n",
    "    chunks = split_into_token_chunks(full_text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"collateral\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i,chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "def chunk_powerpoint(doc: Dict, doc_id: str) -> List[Dict]:\n",
    "    title = doc.get(\"file_name\", f\"ppt_{doc_id}\")\n",
    "    date = doc.get(\"metadata\", {}).get(\"Created\", \"\")\n",
    "    text_blocks = [item.get(\"text\", \"\") for item in doc.get(\"text_data\", [])]\n",
    "    full_text = clean_text(\" \".join(text_blocks))\n",
    "    chunks = split_into_token_chunks(full_text)\n",
    "\n",
    "    return [{\n",
    "        \"text\": chunk,\n",
    "        \"source\": \"powerpoints\",\n",
    "        \"title\": title,\n",
    "        \"doc_id\": doc_id,\n",
    "        \"chunk_id\": i,\n",
    "        \"date\": date\n",
    "    } for i, chunk in enumerate(chunks)]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl_file(file_path: str, chunk_fn, source: str, output_path: str):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        docs = [json.loads(line.strip())for line in f if line.strip()]\n",
    "\n",
    "    all_chunks=[]\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_id=f\"{source}_{i:03d}\"\n",
    "        all_chunks.extend(chunk_fn(doc,doc_id))\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in all_chunks:\n",
    "            f.write(json.dumps(chunk) + '\\n')\n",
    "        print(f\"Saved {len(all_chunks)} chunks to {output_path}\")\n",
    "\n",
    "\n",
    "def process_folder(folder_path: str, chunk_fn, source: str, output_path: str):\n",
    "    all_chunks = []\n",
    "    for i, file in enumerate(Path(folder_path).glob(\"*.json\")):\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            doc = json.load(f)\n",
    "            doc_id = file.stem\n",
    "            all_chunks.extend(chunk_fn(doc, doc_id))\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for chunk in all_chunks:\n",
    "            f.write(json.dumps(chunk) + '\\n')\n",
    "    print(f\"Saved {len(all_chunks)} chunks to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1354 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_blog.jsonl\n",
      "Saved 93 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_collateral.jsonl\n",
      "Saved 49 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_powerpoint.jsonl\n",
      "Saved 392 chunks to /Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_grants.jsonl\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('/Users/sharvari/Downloads/CAFB_Challenge/outputs', exist_ok=True)\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/blog_posts.jsonl',\n",
    "    chunk_blog_post,\n",
    "    'blog_posts',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_blog.jsonl'\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/collateral.jsonl',\n",
    "    chunk_collateral,\n",
    "    'collateral',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_collateral.jsonl'\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/powerpoints.jsonl',\n",
    "    chunk_powerpoint,\n",
    "    'powerpoints',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_powerpoint.jsonl'\n",
    ")\n",
    "\n",
    "process_jsonl_file(\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/data/grant_proposals.jsonl',\n",
    "    chunk_grant_proposal,\n",
    "    'grant_proposals',\n",
    "    '/Users/sharvari/Downloads/CAFB_Challenge/outputs/chunks_grants.jsonl'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/blog_posts.jsonl',\n",
    "#         chunk_blog_post,\n",
    "#         'blog_posts',\n",
    "#         'outputs/chunks_blog.jsonl'\n",
    "#     )\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/collateral.jsonl',\n",
    "#         chunk_collateral,\n",
    "#         'collateral',\n",
    "#         'outputs/chunks_collateral.jsonl'\n",
    "#     )\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/powerpoints.jsonl',\n",
    "#         chunk_powerpoint,\n",
    "#         'powerpoints',\n",
    "#         'outputs/chunks_powerpoints.jsonl'\n",
    "#     )\n",
    "\n",
    "#     process_jsonl_file(\n",
    "#         'data/grants_proposals.jsonl',\n",
    "#         chunk_grant_proposal,\n",
    "#         'grant_proposals',\n",
    "#         'outputs/chunks_grants.jsonl'\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafb-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
